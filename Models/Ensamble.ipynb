{"cells":[{"cell_type":"markdown","metadata":{"id":"5ggS-gkiABsV"},"source":["# IMPORTS"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import sys\n","import os\n","dir_path = os.getcwd().split(os.path.sep)\n","root_index = dir_path.index('Machine_Learning_project')\n","root_path = os.path.sep.join(dir_path[:root_index + 1])\n","sys.path.append(root_path + '/code/')\n","sys.path.append(root_path + '/code/data_loaders/')\n","sys.path.append(root_path + '/code/Nostra_Neural_Network/')\n","sys.path.append(root_path + '/code/data_predictions/')\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import mean_squared_error\n","from Nostra_Neural_Network.metrics import *\n","from data_cup import *\n","import pandas as pd\n","from metrics import *\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"s8k5GMWSlwFV"},"source":["### Ensemble Techniques Exploration\n","In this notebook, we explore ensemble techniques to amalgamate predictions from our most effective models, which were trained with the best-found hyperparameters during the model selection phase.\n","\n","## Ensemble Strategies Explored\n","We examine two ensemble strategies:\n","\n","1. **Arithmetic Average**\n","The arithmetic average entails aggregating predictions from each individual model by computing their mean.\n","\n","2. **Weighted Average by performance**\n","The weighted average involves assigning distinct weights to each model's predictions before computing the aggregate. We chose to average the predictions of the different neural networks first, and then average this combined prediction with the SVM predictions. This approach aimed to leverage the complementary strengths of neural networks and SVMs, potentially enhancing the overall predictive performance of the ensemble.\n","\n","## Experimental Approach\n","We conducted experiments by averaging several models making predictios in the validation set. It's worth noting that neural networks trained with the best-found hyperparameters during the model selection phase have already undergone an averaging process across five trials, specifically for SGD and Adam optimizers.\n","\n","\n","Additionally, the neural network trained using the gradient descent (GD) optimizer utilized a learning scheduler that required a validation set for training. Consequently, we split the training set into 10 folds, and for each fold, we trained the neural network on the remaining 9 folds. We then averaged the predictions made by each of these models on the test set\n","\n",".\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1i9Z19l5AWNN"},"source":["# DATA SPLITTING"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":260,"status":"ok","timestamp":1716062748573,"user":{"displayName":"ANTONIO BRAU","userId":"08808754285343381356"},"user_tz":-120},"id":"qoUZEfdsqDgi"},"outputs":[],"source":["\n","df=cup = CupDataset('Cup_tr').data\n","y=df.iloc[:,-3:]\n","x=df.iloc[:,1:-3]\n","\n","\n","X_TR, X_TS, Y_TR, Y_TS = train_test_split(x, y, test_size=1/10, random_state=0)\n","X_TR, X_VL, Y_TR, Y_VL = train_test_split(X_TR, Y_TR, test_size=0.111, random_state=0)\n","\n","Y_TR = Y_TR.values\n","Y_TS = Y_TS.values\n","Y_VL = Y_VL.values\n"]},{"cell_type":"markdown","metadata":{"id":"59fPxyFKAwmG"},"source":["# FINAL MODEL SELECTION\n","In this stage, we aggregate validation predictions from multiple models, computing both arithmetic and weighted averages. These models were trained on an initial training set comprising 800 examples and made predictions on a validation set containing 100 examples.\n","\n","This step is crucial as it allows us to compare the performance of various models and ensemble approaches introduced earlier. Subsequently, we analyze the Mean Euclidean Error (MEE) alongside its standard deviation for each individual model and ensemble approach, aiding us in selecting the best-performing model.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","data_predictions_path = root_path + '/Models/data_predictions/'\n","\n","\n","\n","Y_SVM_train = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_train_VL_svm.csv\"), delimiter=\",\")\n","Y_SVM_test = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_test_VL_svm.csv\"), delimiter=\",\")\n","\n","Y_ADAM_train = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_train_VL_scikit.csv\"), delimiter=\",\")\n","Y_ADAM_test = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_test_VL_scikit.csv\"), delimiter=\",\")\n","\n","Y_GD_train = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_train_VL_Antonio.csv\"), delimiter=\",\")\n","Y_GD_test = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_test_VL_Antonio.csv\"), delimiter=\",\")\n","\n","Y_SGD_train = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_train_VL_SGD.csv\"), delimiter=\",\")\n","Y_SGD_test = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_test_VL_SGD.csv\"), delimiter=\",\")\n","\n","# Compute the arithmetic average of all model predictions\n","Y_AVG_train = np.mean([Y_SVM_train, Y_ADAM_train, Y_GD_train, Y_SGD_train], axis=0)\n","Y_AVG_test = np.mean([Y_SVM_test, Y_ADAM_test, Y_GD_test, Y_SGD_test], axis=0)\n","\n","# Compute the weighted average: average of neural networks first, then average with SVM\n","Y_NN_AVG_train = np.mean([Y_GD_train, Y_ADAM_train, Y_SGD_train], axis=0)\n","Y_NN_AVG_test = np.mean([Y_GD_test, Y_ADAM_test, Y_SGD_test], axis=0)\n","Y_W_AVG_train = np.mean([Y_NN_AVG_train, Y_SVM_train], axis=0)\n","Y_W_AVG_test = np.mean([Y_NN_AVG_test, Y_SVM_test], axis=0)\n","\n","def format_metric(metric, std):\n","    return f\"{metric:.2f} +- {std:.2f}\"\n","\n","def MSE(y, d):\n","    \n","    return mean_squared_error(y, d)\n","\n","def MSE_std(y, d):\n","    mse = mean_squared_error(y, d)\n","    squared_diff = np.square(y - d)\n","    mse_std = np.sqrt(np.mean(np.square(squared_diff - mse)))\n","    return mse_std\n","print(\"--------------- Validation Train ---------------\")\n","print(\"SGD MEE TRAIN:  \", format_metric(MEE(Y_SGD_train, Y_TR), MEE_std(Y_SGD_train, Y_TR)), \"SGD MSE TRAIN:  \", format_metric(MSE(Y_SGD_train, Y_TR), MSE_std(Y_SGD_train, Y_TR)))\n","print(\"SVM MEE TRAIN:  \", format_metric(MEE(Y_SVM_train, Y_TR), MEE_std(Y_SVM_train, Y_TR)), \"SVM MSE TRAIN:  \", format_metric(MSE(Y_SVM_train, Y_TR), MSE_std(Y_SVM_train, Y_TR)))\n","print(\"MLP ADAM MEE TRAIN:  \", format_metric(MEE(Y_ADAM_train, Y_TR), MEE_std(Y_ADAM_train, Y_TR)), \"MLP ADAM MSE TRAIN:  \", format_metric(MSE(Y_ADAM_train, Y_TR), MSE_std(Y_ADAM_train, Y_TR)))\n","print(\"MLP GD scratch MEE TRAIN:  \", format_metric(MEE(Y_GD_train, Y_TR), MEE_std(Y_GD_train, Y_TR)), \"MLP GD scratch MSE TRAIN:  \", format_metric(MSE(Y_GD_train, Y_TR), MSE_std(Y_GD_train, Y_TR)))\n","print(\"Arithmetic avg MEE TRAIN:  \", format_metric(MEE(Y_AVG_train, Y_TR), MEE_std(Y_AVG_train, Y_TR)), \"Arithmetic avg MSE TRAIN:  \", format_metric(MSE(Y_AVG_train, Y_TR), MSE_std(Y_AVG_train, Y_TR)))\n","print(\"Weighted avg MEE TRAIN:  \", format_metric(MEE(Y_W_AVG_train, Y_TR), MEE_std(Y_W_AVG_train, Y_TR)), \"Weighted avg MSE TRAIN:  \", format_metric(MSE(Y_W_AVG_train, Y_TR), MSE_std(Y_W_AVG_train, Y_TR)))\n","\n","print(\"--------------- Validation Test ---------------\")\n","print(\"SGD MEE TEST:  \", format_metric(MEE(Y_SGD_test, Y_VL), MEE_std(Y_SGD_test, Y_VL)), \"SGD MSE test:  \", format_metric(MSE(Y_SGD_test, Y_VL), MSE_std(Y_SGD_test, Y_VL)))\n","print(\"SVM MEE TEST:  \", format_metric(MEE(Y_SVM_test, Y_VL), MEE_std(Y_SVM_test, Y_VL)), \"SVM MSE test:  \", format_metric(MSE(Y_SVM_test, Y_VL), MSE_std(Y_SVM_test, Y_VL)))\n","print(\"MLP ADAM MEE TEST:  \", format_metric(MEE(Y_ADAM_test, Y_VL), MEE_std(Y_ADAM_test, Y_VL)), \"MLP ADAM MSE test:  \", format_metric(MSE(Y_ADAM_test, Y_VL), MSE_std(Y_ADAM_test, Y_VL)))\n","print(\"MLP GD scratch MEE TEST:  \", format_metric(MEE(Y_GD_test, Y_VL), MEE_std(Y_GD_test, Y_VL)), \"MLP GD scratch MSE test:  \", format_metric(MSE(Y_GD_test, Y_VL), MSE_std(Y_GD_test, Y_VL)))\n","print(\"Arithmetic avg MEE TEST:  \", format_metric(MEE(Y_AVG_test, Y_VL), MEE_std(Y_AVG_test, Y_VL)), \"Arithmetic avg MSE test:  \", format_metric(MSE(Y_AVG_test, Y_VL), MSE_std(Y_AVG_test, Y_VL)))\n","print(\"Weighted avg MEE TEST:  \", format_metric(MEE(Y_W_AVG_test, Y_VL), MEE_std(Y_W_AVG_test, Y_VL)), \"Weighted avg MSE test:  \", format_metric(MSE(Y_W_AVG_test, Y_VL), MSE_std(Y_W_AVG_test, Y_VL)))\n"]},{"cell_type":"markdown","metadata":{"id":"zVPlh7XOETNo"},"source":["# MODEL ASSESSTEMENT\n","\n","In this section, we evaluate the performance of our best model, which is the weighted average ensemble approach. This model demonstrated the best performance during the validation phase, closely followed by the arithmetic average ensemble. For thoroughness, we also test the individual models on the internal test set to compare their performance and ensure the robustness of our ensemble approach.\n","\n","In this phase, all models were retrained on the combined training set, which includes both the initial training set and the validation set used during the model selection phase."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":272,"status":"ok","timestamp":1716058878793,"user":{"displayName":"ANTONIO BRAU","userId":"08808754285343381356"},"user_tz":-120},"id":"skaKOYDIPbGr","outputId":"69886952-a852-4325-e569-7c9aeaf21e62"},"outputs":[],"source":["# Load test set predictions from different models\n","import os\n","\n","data_predictions_path = root_path + '/Models/data_predictions/'\n","\n","Y_SVM = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_internal_TS_SVR.csv\"), delimiter=\",\")\n","Y_ADAM = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_internal_TS_scikit.csv\"), delimiter=\",\")\n","Y_GD = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_internal_TS_Antonio.csv\"), delimiter=\",\")\n","Y_SGD = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_internal_TS_SGD.csv\"), delimiter=\",\")\n","\n","\n","# Compute the arithmetic average of all model predictions\n","Y_AVG = np.mean([Y_SVM, Y_ADAM, Y_GD, Y_SGD], axis=0)\n","\n","# Compute the weighted average: average of selected models first, then average with SVM\n","Y_W_AVG = np.mean([np.mean([Y_GD, Y_ADAM, Y_SGD], axis=0), Y_SVM], axis=0)\n","\n","# Define a helper function to format MEE and MEE_std with two decimal places\n","def format_metric(metric, std):\n","    return f\"{metric:.3f} +- {std:.3f}\"\n","\n","print(\"--------------- MEAN SQUARED ERROR ---------------\")\n","print(\"SGD : \", format_metric(mean_squared_error(Y_SGD, Y_TS), MSE_std(Y_SGD, Y_TS)))\n","print(\"SVM : \", format_metric(mean_squared_error(Y_SVM, Y_TS), MSE_std(Y_SVM, Y_TS)))\n","print(\"MLP ADAM : \", format_metric(mean_squared_error(Y_ADAM, Y_TS), MSE_std(Y_ADAM, Y_TS)))\n","print(\"MLP GD scratch : \", format_metric(mean_squared_error(Y_GD, Y_TS), MSE_std(Y_GD, Y_TS)))\n","print(\"Arithmetic : \", format_metric(mean_squared_error(Y_AVG, Y_TS), MSE_std(Y_AVG, Y_TS)))\n","print(\"Weighted avg : \", format_metric(mean_squared_error(Y_W_AVG, Y_TS), MSE_std(Y_W_AVG, Y_TS)))\n","\n","\n","# Evaluate and print MEE and its standard deviation for each model and the ensemble approaches\n","print(\"--------------- MEAN EUCLIDEAN ERROR ---------------\")\n","print(\"SGD : \", format_metric(MEE(Y_SGD, Y_TS), MEE_std(Y_SGD, Y_TS)))\n","print(\"SVM : \", format_metric(MEE(Y_SVM, Y_TS), MEE_std(Y_SVM, Y_TS)))\n","print(\"MLP ADAM : \", format_metric(MEE(Y_ADAM, Y_TS), MEE_std(Y_ADAM, Y_TS)))\n","print(\"MLP GD scratch : \", format_metric(MEE(Y_GD, Y_TS), MEE_std(Y_GD, Y_TS)))\n","print(\"Arithmetic : \", format_metric(MEE(Y_AVG, Y_TS), MEE_std(Y_AVG, Y_TS)))\n","print(\"Weighted avg : \", format_metric(MEE(Y_W_AVG, Y_TS), MEE_std(Y_W_AVG, Y_TS)))\n"]},{"cell_type":"markdown","metadata":{"id":"9EPiJdTNJoxB"},"source":["# FINAL RETRAINING AND BLIND TEST PREDICTIONS\n","Since we've already estimated the test error using the internal test set, we can proceed with a final re-training on all our development data. This ensures we utilize all available information for training without introducing bias from data leakage.\n","\n","Based on our previous validation, the ensemble approach using the Weighted Average has shown the best performance. Therefore, we'll use this ensemble model as our final model for generating predictions on the blind test set"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":718,"status":"ok","timestamp":1716063007260,"user":{"displayName":"ANTONIO BRAU","userId":"08808754285343381356"},"user_tz":-120},"id":"m56Tgye4Q0C2"},"outputs":[],"source":["data_predictions_path = root_path + '/Models/data_predictions/'\n","\n","Y_SVM = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_internal_TS_SVR.csv\"), delimiter=\",\")\n","Y_ADAM = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_internal_TS_scikit.csv\"), delimiter=\",\")\n","Y_GD = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_internal_TS_Antonio.csv\"), delimiter=\",\")\n","Y_SGD = np.loadtxt(os.path.join(data_predictions_path, \"y_pred_internal_TS_SGD.csv\"), delimiter=\",\")\n","\n","# Compute the weighted average: average of selected models first, then average with SVM\n","Y_W_AVG = np.mean([np.mean([Y_GD, Y_ADAM, Y_SGD], axis=0), Y_SVM], axis=0)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3BNmHRDXV-3p"},"source":["# STORING BLIND RESULTS"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["import os\n","\n","data_predictions_path = root_path + \"/Models/datapredictions/\"\n","\n","# Store final blind test mean predictions\n","with open(os.path.join(data_predictions_path, 'BURDOS_ML-CUP23-TS.csv'), 'w') as outf:\n","    # Team Info\n","    outf.write(\"# Silvio Calderaro, Salvatore Ergoli, Antonio Brau\\n\")\n","    outf.write(\"# BURDOS\\n\")\n","    outf.write(\"# ML-CUP23 \\n\")\n","    outf.write(\"# 20/05/2024\\n\")\n","\n","    # Writing predictions\n","    for i, pred in enumerate(Y_W_AVG, 1):\n","        outf.write(f\"{i},{','.join(map(str, pred))}\\n\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPijKqJRNkRqGE2wD8t2HtH","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
