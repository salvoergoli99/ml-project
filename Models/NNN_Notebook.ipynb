{"cells":[{"cell_type":"markdown","metadata":{"id":"g6XuLVmkVvpd"},"source":["# Import Files and Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np\n","\n","\n","\n","dir_path = os.getcwd().split(os.path.sep)\n","root_index = dir_path.index('Machine_Learning_project')\n","root_path = os.path.sep.join(dir_path[:root_index + 1])\n","sys.path.append(root_path + '/code/')\n","sys.path.append(root_path + '/code/data_loaders/')\n","sys.path.append(root_path + '/code/utils_visualizations/')\n","\n","\n","from data import *\n","from data_cup import *\n","from Nostra_Neural_Network import *\n","from Nostra_Neural_Network.Batch import Batch\n","from Nostra_Neural_Network.Train import Train\n","from Nostra_Neural_Network.K_FOLD import k_fold\n","from Nostra_Neural_Network.NN_creation import NN_creation\n","from Nostra_Neural_Network.Average import Average\n","from Nostra_Neural_Network.Neural_Network import Neural_Network\n","from Nostra_Neural_Network.Activation_Function import activation_function\n","from plot import *\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.metrics import accuracy_score, mean_squared_error\n","from sklearn.preprocessing import OneHotEncoder\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jMRYdfiSVvpg"},"source":["# **NEURAL NETWORK**\n","\n","#### In this project, we have developed a neural network in Python, specifically a fully connected Multi-Layer Perceptron (MLP) that utilizes a backpropagation algorithm for training. The network is designed to be highly flexible, allowing for a general number of layers and units per layer. The training algorithm has been implemented exclusively using computational libraries such as NumPy. Key features of our implementation include:\n","\n","## - **OPTIMIZERS** :\n","#### The network can be trained using the following optimizers:\n","- ##### **Batch Gradient Descent (GD)** : The most basic and widely used optimization algorithm.\n","- ##### **Stocastich Gradient Descent (SGD)** : Simple yet effective optimization algorithm, the network supports variable batch sizes and shuffles the training data at each epoch to ensure that the model generalizes well and does not overfit to the order of the data.\n","- ##### **ADAM Optimizer**: The network can also be trained using the Adam optimizer, which combines the advantages of both Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp), providing an efficient and effective optimization method.\n","\n","## - **Regularization Techniques**:\n","#### We have integrated several regularization techniques to help prevent overfitting and improve generalization:\n","##### - **L2 Regularization**: Also known as weight decay, this technique helps prevent overfitting by penalizing large weights, encouraging the network to maintain smaller and more stable weights.\n","##### - **L1 Regularization**: This technique promotes sparsity by penalizing the absolute values of the weights, which can lead to some weights becoming zero and thereby creating a sparse network.\n","##### - **Early Stopping**: Early stopping halts the training process when the performance on a validation set starts to deteriorate, preventing overfitting and saving computational resources.\n","\n","## **Additional Features**:\n","##### - **Momentum**: To accelerate the convergence of the gradient descent algorithm, we have implemented momentum, which helps the network navigate through ravines and avoid local minima more effectively.\n","##### - **Gradient Clipping**: We have implemented gradient thresholding to prevent exploding gradients, ensuring that the updates to the model parameters remain stable.\n","Activation Functions: The implementation includes a variety of activation functions such as tanh, sigmoid, ReLU, etc., allowing users to experiment with different non-linearities in the network.\n","Loss Functions: We have incorporated several loss functions, including Huber Loss and Mean Squared Error (MSE), providing flexibility in optimizing the network for different types of regression tasks.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yoig3ErRVvpg"},"source":["##"]},{"cell_type":"markdown","metadata":{"id":"LX6DvXU3Vvpg"},"source":["\n","\n","Data Split:\n","\n","- TR (Training Set): This dataset consists of 80% of the total data and is used to train the machine learning model.\n","\n","- VL (Validation Set): This dataset comprises 10% of the total data and is used to evaluate the model's performance.\n","\n","- TS (Test Set): This dataset makes up the remaining 10% of the total data and is used for evaluation of the final model's performance.\n","\n","\n","Data Preprocessing:\n","\n","- Polynomial Feature (Degree 2): We create second-degree polynomial terms of the original features to capture non-linear relationships.\n","\n","- Arctanh Normalization: We apply the arctanh function to normalize the data for improved training convergence and feature scaling."]},{"cell_type":"markdown","metadata":{"id":"0KukarUJVvpg"},"source":["# BASE MODEL CREATION"]},{"cell_type":"markdown","metadata":{"id":"Q5gABGTyVvph"},"source":["## SGD"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"juEjQY9ZVvph"},"outputs":[],"source":["\n","# This function trains a neural network using the SGD (Stochastic Gradient Descent) optimization algorithm.\n","# Parameters:\n","#   - X_TR: array-like, training set features.\n","#   - Y_TR: array-like, training set target.\n","#   - X_TS: array-like, test set features.\n","#   - Y_TS: array-like, test set target.\n","#   - epoche: int, number of training epochs.\n","#   - learning_rate: float, learning rate.\n","#   - lamda: float, L2 regularization parameter.\n","#   - alfa: float, momentum parameter.\n","#   - batch_size: int, mini-batch size.\n","\n","def MLP_SGD(X_TR, Y_TR, X_TS, Y_TS, epochs, learning_rate, lamda, alfa, batch_size):\n","    # Create the neural network object\n","    nn = Neural_Network(1, 17)\n","    nn.layers[0].activation_function = activation_function(\"sigmoid\")\n","    nn.add_layer(4, activation=\"tanh\")\n","    nn.gradient_treshold = -1\n","    nn.batch_size = 16\n","    nn.lamda_2 = lamda\n","    nn.alfa = alfa\n","    nn.Inizialization(type=\"Xavier_uniform\", scale=1)\n","\n","    # Set the training data\n","    nn.set_train(X_TR, Y_TR, batch_size=batch_size, Loss_Function=\"MSE\", random_state=1)\n","\n","    # Lists to store performance metrics\n","    accuracy_list_TR = []\n","    accuracy_list_TS = []\n","    MSE_list_TR = []\n","    MSE_list_TS = []\n","\n","    # Training loop\n","    iter_epoch = X_TR.shape[0]//nn.batch_size if nn.batch_size!= -1 else 1\n","    for i in range(epochs):\n","        [ nn.step_train(learning_rate) for j in range(iter_epoch) ]\n","        y_pred_train = nn.forward(X_TR)\n","        y_pred_test = nn.forward(X_TS)\n","\n","        # Compute performance metrics\n","        accuracy_list_TR.append(accuracy_score(Y_TR, np.round(y_pred_train)))\n","        accuracy_list_TS.append(accuracy_score(Y_TS, np.round(y_pred_test)))\n","        MSE_list_TR.append(mean_squared_error(Y_TR, y_pred_train))\n","        MSE_list_TS.append(mean_squared_error(Y_TS, y_pred_test))\n","\n","\n","    # Return performance metrics\n","    return accuracy_list_TR, accuracy_list_TS, MSE_list_TR, MSE_list_TS\n"]},{"cell_type":"markdown","metadata":{"id":"QyeJ97pDVvph"},"source":["## ADAM"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_KeiO42EVvph"},"outputs":[],"source":["\n","# This function trains a neural network using the SGD (Stochastic Gradient Descent) optimization algorithm.\n","# Parameters:\n","#   - X_TR: array-like, training set features.\n","#   - Y_TR: array-like, training set target.\n","#   - X_TS: array-like, test set features.\n","#   - Y_TS: array-like, test set target.\n","#   - epoche: int, number of training epochs.\n","#   - learning_rate: float, learning rate.\n","#   - lamda: float, L2 regularization parameter.\n","#   - alfa: float, momentum parameter.\n","#   - batch_size: int, mini-batch size.\n","\n","def MLP_ADAM(X_TR, Y_TR, X_TS, Y_TS, epochs, learning_rate, lamda, batch_size):\n","    # Create the neural network object\n","    nn = Neural_Network(1, 17, adam = True)\n","    nn.layers[0].activation_function = activation_function(\"sigmoid\")\n","    nn.add_layer(4, activation=\"tanh\")\n","    nn.gradient_treshold = 10\n","    nn.batch_size = 16\n","    nn.lamda_2 = lamda\n","    nn.beta_1 = 0.9\n","    nn.beta_2 = 0.999\n","    nn.Inizialization(type=\"Xavier_uniform\", scale=1)\n","\n","    # Set the training data\n","    nn.set_train(X_TR, Y_TR, batch_size=batch_size, Loss_Function=\"MSE\", random_state = None)\n","\n","    # Lists to store performance metrics\n","    accuracy_list_TR = []\n","    accuracy_list_TS = []\n","    MSE_list_TR = []\n","    MSE_list_TS = []\n","\n","    iter_epoch = X_TR.shape[0]//nn.batch_size if nn.batch_size!= -1 else 1\n","    # Training loop\n","    for i in range(epochs):\n","        [ nn.step_train(learning_rate) for j in range(iter_epoch) ]\n","        y_pred_train = nn.forward(X_TR)\n","        y_pred_test = nn.forward(X_TS)\n","\n","        # Compute performance metrics\n","        accuracy_list_TR.append(accuracy_score(Y_TR, np.round(y_pred_train)))\n","        accuracy_list_TS.append(accuracy_score(Y_TS, np.round(y_pred_test)))\n","        MSE_list_TR.append(mean_squared_error(Y_TR, y_pred_train))\n","        MSE_list_TS.append(mean_squared_error(Y_TS, y_pred_test))\n","\n","    # Return performance metrics\n","    return accuracy_list_TR, accuracy_list_TS, MSE_list_TR, MSE_list_TS"]},{"cell_type":"markdown","metadata":{"id":"0dTfo9c9Vvph"},"source":["# MONK 1"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"6g3ZqlcPVvph"},"outputs":[],"source":["\n","m1_train = MonksDataset('monk1_train')\n","m1_test= MonksDataset('monk1_test')\n","\n","#Splitting the data into train, and test sets\n","X_TR_m1, Y_TR_m1, X_TS_m1, Y_TS_m1 = get_monks_data(m1_train, m1_test)\n","\n","Y_TS_m1 = Y_TS_m1.values\n","Y_TR_m1 = Y_TR_m1.values\n","X_TR_m1 = X_TR_m1.values\n","X_TS_m1 = X_TS_m1.values\n","\n","# Encoding with the O.H.E. method using pandas get_dummies()\n","encoder = OneHotEncoder()\n","X_TR_m1 = encoder.fit_transform(X_TR_m1).toarray()\n","X_TS_m1= encoder.transform(X_TS_m1).toarray()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"kPRZathyVvpi"},"outputs":[],"source":["\n","accuracy_TR, accuracy_TS, MSE_TR, MSE_TS= MLP_SGD(X_TR_m1, Y_TR_m1, X_TS_m1, Y_TS_m1, epochs = 300, learning_rate = 0.8, alfa=0.8, batch_size = 8, lamda = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtXs6E-rVvpi","outputId":"af26c294-48e4-45f7-9a4b-408636d83f71"},"outputs":[],"source":["plot_history_monk('Monk 1', accuracy_TR, accuracy_TS, MSE_TR, MSE_TS)\n","print('Monk 1', accuracy_TR[-1], accuracy_TS[-1], MSE_TR[-1], MSE_TS[-1])"]},{"cell_type":"markdown","metadata":{"id":"5yJaIN1JVvpi"},"source":["# MONK 2"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"rE9HJVhYVvpi"},"outputs":[],"source":["current_directory = os.getcwd()\n","data_directory = os.path.join(current_directory, \"DATA\")\n","m2_train = MonksDataset('monk2_train')\n","m2_test= MonksDataset('monk2_test')\n","\n","#Splitting the data into train, and test sets\n","X_dev, Y_TR_m2, X_test_m2, Y_TS_m2 = get_monks_data(m2_train, m2_test)\n","\n","Y_TS_m2 = Y_TS_m2.values\n","Y_TR_m2 = Y_TR_m2.values\n","\n","# Encoding with the O.H.E. method using pandas get_dummies()\n","X_TR_m2 = pd.get_dummies(X_dev, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6']).values\n","X_TS_m2= pd.get_dummies(X_test_m2, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6']).values"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"NDuJMpDyVvpi","outputId":"4d01bd99-f373-436d-8248-cb7a7d93ef6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Monk 2 1.0 1.0 3.238688992054987e-05 4.648470641720167e-05\n"]}],"source":["\n","accuracy_TR, accuracy_TS, MSE_TR, MSE_TS= MLP_SGD(X_TR_m2, Y_TR_m2, X_TS_m2, Y_TS_m2, epochs = 300, learning_rate = 0.9, batch_size = 1, lamda = 0, alfa = 0.8)\n","print('Monk 2', accuracy_TR[-1], accuracy_TS[-1], MSE_TR[-1], MSE_TS[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6Zu0-HKVvpi","outputId":"92322336-67bf-4bcc-fea5-a8613edcc2ac"},"outputs":[],"source":["plot_history_monk('Monk 2', accuracy_TR, accuracy_TS, MSE_TR, MSE_TS)"]},{"cell_type":"markdown","metadata":{"id":"2s9WaUfSVvpj"},"source":["# MONK 3"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"wJaYmZcNVvpj"},"outputs":[],"source":["current_directory = os.getcwd()\n","data_directory = os.path.join(current_directory, \"DATA\")\n","m3_train = MonksDataset('monk3_train')\n","m3_test= MonksDataset('monk3_test')\n","\n","#Splitting the data into train, and test sets\n","X_dev, Y_TR_m3, X_test_m3, Y_TS_m3 = get_monks_data(m3_train, m3_test)\n","\n","Y_TS_m3 = Y_TS_m3.values\n","Y_TR_m3 = Y_TR_m3.values\n","\n","# Encoding with the O.H.E. method using pandas get_dummies()\n","X_TR_m3 = pd.get_dummies(X_dev, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6']).values\n","X_TS_m3= pd.get_dummies(X_test_m3, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6']).values"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"_IfpP8E4Vvpj","outputId":"56e0e6cd-101f-41ed-e18a-38443a1b34cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Monk 2 0.9426229508196722 0.9675925925925926 0.056040645327578936 0.04865039909749305\n"]}],"source":["\n","accuracy_TR, accuracy_TS, MSE_TR, MSE_TS= MLP_ADAM(X_TR_m3, Y_TR_m3, X_TS_m3, Y_TS_m3, epochs = 300, learning_rate = 0.01, batch_size = 8, lamda = 0.5)\n","print('Monk 2', accuracy_TR[-1], accuracy_TS[-1], MSE_TR[-1], MSE_TS[-1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eKaB-fcVvpj","outputId":"40982795-c927-4264-cc74-655af1b8f993"},"outputs":[],"source":["plot_history_monk('Monk 3', accuracy_TR, accuracy_TS, MSE_TR, MSE_TS)"]},{"cell_type":"markdown","metadata":{"id":"JtNPxuSSV4P-"},"source":["# CUP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgVNwxMEVvpj"},"outputs":[],"source":["# Create an instance of the dataset for Cup training and Cup test\n","cup = CupDataset('Cup_tr')\n","\n","\n","\n","df =cup.data\n","\n","\n","\n","\n","\n","#df=pd.read_csv(\"DATA/ML-CUP23-TR.csv\",comment=\"#\",header=None)\n","\n","y=df.iloc[:,-3:]\n","x=df.iloc[:,1:-3]\n","\n","#PRIMO SPLITTING DEI DATI PER INTERNAL TEST SET\n","X_TR, X_TS, Y_TR, Y_TS = train_test_split(x, y, test_size=0.1, random_state=0)\n","\n","#SECONDO SPLITTING DEI DATI PER MODEL ASSESSTEMENT\n","X_TR, X_VL, Y_TR, Y_VL = train_test_split(X_TR, Y_TR, test_size=0.111,random_state=0)\n","\n","X_TR = X_TR.values\n","X_TS = X_TS.values\n","Y_TR = Y_TR.values\n","Y_TS = Y_TS.values\n","\n","\n","#Generate polynomial features\n","poly = PolynomialFeatures(2)\n","X_TR = poly.fit_transform(X_TR)\n","X_TS = poly.transform(X_TS)\n","X_VL = poly.transform(X_VL)\n","\n","#Apply arctanh normalization to numerical features\n","X_TR = np.arctanh(X_TR[:,1:])\n","X_TS = np.arctanh(X_TS[:,1:])\n","X_VL = np.arctanh(X_VL[:,1:])"]},{"cell_type":"markdown","metadata":{"id":"qxvQegxjVvpj"},"source":["## Hyper-parameters Tuning\n","A common approach is to start with a coarse search across a wide range of values to find promising sub-ranges of our parameter space. Then, you would zoom into these ranges and perform another search to fine-tune the configurations.\n","\n","Here, we proceed as follows:\n","1. (coarse) Grid-search across a wide range of hyper-paramaters and values;\n","2. (fine-tune) Random-search into zoomed intervals w.r.t. best configuration found by grid-search.\n","\n","Then, we perform a single run of grid-search and random-search with the respectively best configurations while taking into account a PolynomialFeatures pre-processing with fixed degree. The best configurations that will be used for final re-training and evaluation on internal test is the one with the best mean MEE on the validation cross-validation.\n","\n","Note that, tuning of the polynomial degree wasn't performed because it would be very expensive. Thus, we simply decided to use a fixed degree value."]},{"cell_type":"markdown","metadata":{"id":"9Olw0K6HVvpj"},"source":["## Grid Search"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"gyoUvBYSVvpk"},"outputs":[],"source":["from itertools import product\n","def k_fold_grid_search(X, Y, grid, epoche, k):\n","    \"\"\"\n","    This function performs a K-Fold Grid Search on a Neural Network model. It takes a grid of hyperparameters, the number of training epochs, and the number of folds (k) as input.\n","\n","    Args:\n","        grid (dict): A dictionary containing the grid of hyperparameters to search. Keys are hyperparameter names, values are lists of possible values.\n","        epoche (int): The number of training epochs for the Neural Network.\n","        k (int): The number of folds to use in K-Fold Cross Validation.\n","\n","    Returns:\n","        dict: A dictionary containing the results of the Grid Search. Keys are hyperparameter names, values are lists of the searched values. Additionally, keys prefixed with \"MEE_TR_\", \"MEE_TS_\", \"MSE_TR_\", \"MSE_TS_\", and \"MEE_std_\" contain the corresponding Mean Absolute Error (MAE) and Mean Squared Error (MSE) for training and validation sets, along with the standard deviation of the validation MAE for each combination of hyperparameters.\n","    \"\"\"\n","    # KFold object iterates through folds\n","    fold = k_fold(k, X, Y)\n","\n","    # Create a dictionary to store results for each hyperparameter combination\n","    dic_df = {_: [] for _ in grid}\n","\n","    # Initialize additional metrics for tracking\n","    dic_df[\"MEE_TR\"] = []\n","    dic_df[\"MEE_TR\"]=[]\n","    dic_df[\"MEE_TS\"]=[]\n","    dic_df[\"MSE_TR\"]=[]\n","    dic_df[\"MSE_TS\"]=[]\n","    dic_df[\"MEE_std\"]=[]\n","\n","    # Define a default Neural Network configuration\n","    dizionario_NN = NN_creation({}).dic\n","    dizionario_NN[\"NN_shape\"]=(3,65)\n","\n","    while(fold.EO()): # Loop until all folds have been used\n","\n","        # Get the next fold of data for training and validation\n","        _X_TR,_Y_TR,_X_VL,_Y_VL = fold.data()\n","\n","        # Update Neural Network configuration with current fold data\n","        dizionario_NN[\"X_TR\"]=_X_TR\n","        dizionario_NN[\"Y_TR\"]=_Y_TR\n","        dizionario_NN[\"X_TS\"]=_X_VL\n","        dizionario_NN[\"Y_TS\"]=_Y_VL\n","\n","        # Iterate through all combinations of hyperparameter values in the grid\n","        for i,iperparameters in enumerate(product( *[grid[_] for _ in grid ])):\n","            stringa =str(i)+\" addestramento --> \"\n","\n","            # Record each hyperparameter value for this combination\n","            for iperparameter,name_iperparameter in zip(iperparameters,grid):\n","                dic_df[name_iperparameter].append(iperparameter)\n","                dizionario_NN[name_iperparameter] = iperparameter\n","                stringa= stringa +\"  \"+name_iperparameter+\" : \"+str(iperparameter)\n","\n","            # Create a Neural Network instance with current hyperparameters\n","            nn = NN_creation(dizionario_NN)\n","\n","            # Train the Neural Network and get performance metrics\n","            MSE_TR,MSE_TS,MEE_TR,MEE_TS,MEE_TS_std = nn.automatic_learning(epoche = epoche,verbouse=False)\n","\n","            # Store performance metrics for this hyperparameter combination\n","            dic_df[\"MSE_TR\"].append(MSE_TR[-1])\n","            dic_df[\"MSE_TS\"].append(MSE_TS[-1])\n","            dic_df[\"MEE_TR\"].append(MEE_TR[-1])\n","            dic_df[\"MEE_TS\"].append(MEE_TS[-1])\n","            dic_df[\"MEE_std\"].append(MEE_TS_std[-1])\n","\n","            stringa += \"  MEE_TR : \"+str(dic_df[\"MEE_TR\"][-1])+\"  MEE_TS : \"+str(dic_df[\"MEE_TS\"][-1])+\" +- \"+str(dic_df[\"MEE_std\"][-1])\n","            print(stringa)\n","\n","    #Return the results of the Grid Search.\n","    return dic_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DS1ilSPCVvpk","outputId":"510bbc9d-181d-456f-8f92-29885cd48c27"},"outputs":[],"source":["grid= {\"N_layer\":[1,2,3],\"N_units\": [10,30,50,100,150],\"lamda_2\":[0.01,0.001,0.0001],\"alfa\":[0.25,0.5,0.75]}\n","\n","result = k_fold_grid_search( X_TR, Y_TR, grid , epoche = 5000, k = 5)"]},{"cell_type":"markdown","metadata":{"id":"0QcJ-FOvVvpk"},"source":["## Random Search"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"jLMX68wxVvpk"},"outputs":[],"source":["def random_search_kfold(X, Y, random_grid, epoche, k, n_iterazioni):\n","    \"\"\"\n","    This function performs a K-Fold Random Search on a Neural Network model. It takes a grid of hyperparameters, the number of training epochs, and the number of folds (k) as input.\n","\n","    Args:\n","        grid (dict): A dictionary containing the grid of hyperparameters to search. Keys are hyperparameter names, values are lists of possible values.\n","        epoche (int): The number of training epochs for the Neural Network.\n","        k (int): The number of folds to use in K-Fold Cross Validation.\n","\n","    Returns:\n","        dict: A dictionary containing the results of the Random Search. Keys are hyperparameter names, values are lists of the searched values. Additionally, keys prefixed with \"MEE_TR_\", \"MEE_TS_\", \"MSE_TR_\", \"MSE_TS_\", and \"MEE_std_\" contain the corresponding Mean Absolute Error (MAE) and Mean Squared Error (MSE) for training and validation sets, along with the standard deviation of the validation MAE for each combination of hyperparameters.\n","    \"\"\"\n","    # KFold object iterates through folds\n","    fold = k_fold(k, X, Y)\n","\n","    # Create a dictionary to store results for each hyperparameter combination\n","    dic_df = {_: [] for _ in random_grid}\n","\n","    # Initialize additional metrics for tracking\n","    dic_df[\"MEE_TR\"] = []\n","    dic_df[\"MEE_TR\"]=[]\n","    dic_df[\"MEE_TS\"]=[]\n","    dic_df[\"MSE_TR\"]=[]\n","    dic_df[\"MSE_TS\"]=[]\n","    dic_df[\"MEE_std\"]=[]\n","\n","    # Define a default Neural Network configuration\n","    dizionario_NN = NN_creation({}).dic\n","    dizionario_NN[\"NN_shape\"]=(3,65)\n","\n","    for i in range(n_iterazioni):\n","        stringa =str(i)+\" addestramento --> \"\n","        for name_iperparameter in random_grid:\n","            iperparameter = np.random.choice(random_grid[name_iperparameter])\n","            dic_df[name_iperparameter].append(iperparameter)\n","            dizionario_NN[name_iperparameter] = iperparameter\n","            stringa= stringa +\"  \"+name_iperparameter+\" : \"+str(iperparameter)\n","        while(fold.EO()):\n","            stringa1 =\"\"\n","            _X_TR,_Y_TR,_X_VL,_Y_VL = fold.data()\n","            dizionario_NN[\"X_TR\"]=_X_TR\n","            dizionario_NN[\"Y_TR\"]=_Y_TR\n","            dizionario_NN[\"X_TS\"]=_X_VL\n","            dizionario_NN[\"Y_TS\"]=_Y_VL\n","\n","            # Create a Neural Network instance with current hyperparameters\n","            nn = NN_creation(dizionario_NN)\n","\n","            # Train the Neural Network and get performance metrics\n","            MSE_TR,MSE_TS,MEE_TR,MEE_TS,MEE_TS_std = nn.automatic_learning(epoche = epoche,verbouse=False)\n","\n","            # Store performance metrics for this hyperparameter combination\n","            dic_df[\"MSE_TR\"].append(MSE_TR[-1])\n","            dic_df[\"MSE_TS\"].append(MSE_TS[-1])\n","            dic_df[\"MEE_TR\"].append(MEE_TR[-1])\n","            dic_df[\"MEE_TS\"].append(MEE_TS[-1])\n","            dic_df[\"MEE_std\"].append(MEE_TS_std[-1])\n","\n","            stringa1 += \"  MEE_TR : \"+str(dic_df[\"MEE_TR\"][-1])+\"  MEE_TS : \"+str(dic_df[\"MEE_TS\"][-1])+\" +- \"+str(dic_df[\"MEE_std\"][-1])\n","            print(stringa + stringa1)\n","    return dic_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttvvMydbVvpk"},"outputs":[],"source":["random_grid = {\"N_layer\":[1],\"N_units\": [150],\"lamda_2\":np.linspace(0.0003,0.0007,100),\"alfa\":np.linspace(0.15,0.65,100)}\n","dic_df = random_search_kfold(X_TR, Y_TR, random_grid, epoche = 15000, k=5, n_iterazioni=30)"]},{"cell_type":"markdown","metadata":{"id":"dTHJlueIVvpl"},"source":["## Training Best Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"teqzTwBjVvpl"},"outputs":[],"source":["# Settting best iperparameter found in the model selection\n","alfa=0.35\n","lamda = 0.00055\n","batch_size=-1\n","epoche=130000\n","epoche_bath=15000\n","\n","\n","nn=Neural_Network(3,65)\n","nn.add_layer(150,activation= \"tanh\")\n","\n","nn.gradient_treshold = 10\n","nn.lamda_2=lamda\n","nn.alfa=alfa\n","\n","# Create an Average ensemble object with the Neural Network and data\n","avg = Average(nn, X_TR, Y_TR, X_VL,  epoche,epoche_bath)\n","\n","# Get the final prediction from the Average ensemble\n","y_pred = avg.predict()\n","\n","#np.savetxt(\"y_pred_VL_NNN.csv\", y_pred, delimiter=\",\")"]},{"cell_type":"markdown","metadata":{"id":"Mvy36MLyVvpl"},"source":["## Training Best Model for Model Assesstment\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBrhJgkoVvpl","outputId":"a4080579-019f-4573-c588-8946422033d1"},"outputs":[],"source":["# Settting best iperparameter found in the model selection\n","alfa=0.35\n","lamda = 0.00055\n","batch_size=-1\n","epoche=130000\n","epoche_bath=15000\n","\n","\n","nn=Neural_Network(3,65)\n","nn.add_layer(150,activation= \"tanh\")\n","\n","nn.gradient_treshold = 10\n","nn.lamda_2=lamda\n","nn.alfa=alfa\n","\n","X_DEV = np.concatenate((X_TR, X_VL), axis = 0)\n","Y_DEV = np.concatenate((Y_TR, Y_VL), axis = 0)\n","\n","# Create an Average ensemble object with the Neural Network and data\n","avg = Average(nn, X_DEV, Y_DEV, X_TS,  epoche,epoche_bath)\n","\n","# Get the final prediction from the Average ensemble\n","y_pred = avg.predict()\n","\n","#np.savetxt(\"y_pred_internal_TS_NNN.csv\", y_pred, delimiter=\",\")\n"]},{"cell_type":"markdown","metadata":{"id":"lFImRwfMVvpl"},"source":["## Final Retraining"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRUZZ-1cVvpl"},"outputs":[],"source":["X = x.values\n","Y = y.values\n","\n","df1 = pd.read_csv(\"DATA/ML-CUP23-TR.csv\",comment=\"#\",header=None)\n","X_BLIND = df1.iloc[:,1:]\n","\n","# Settting best iperparameter found in the model selection\n","alfa=0.35\n","lamda = 0.00055\n","batch_size=-1\n","epoche=130000\n","epoche_bath=15000\n","\n","\n","nn=Neural_Network(3,65)\n","nn.add_layer(150,activation= \"tanh\")\n","\n","nn.gradient_treshold = 10\n","nn.lamda_2=lamda\n","nn.alfa=alfa\n","\n","# Create an Average ensemble object with the Neural Network and data\n","avg = Average(nn, X, Y, X_BLIND,  epoche,epoche_bath)\n","\n","# Get the final prediction from the Average ensemble\n","y_pred = avg.predict()\n","\n","#np.savetxt(\"y_pred_TS_NNN.csv\", y_pred, delimiter=\",\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
