{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Files and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.optimizers import  SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from tensorflow.keras.initializers import  GlorotUniform, Constant, Zeros, GlorotNormal\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import make_scorer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "dir_path = os.getcwd().split(os.path.sep)\n",
    "root_index = dir_path.index('Machine_Learning_project')\n",
    "root_path = os.path.sep.join(dir_path[:root_index + 1])\n",
    "sys.path.append(root_path + '/code/')\n",
    "sys.path.append(root_path + '/code/data_loaders/')\n",
    "sys.path.append(root_path + '/code/utils_keras')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the Monks datasets\n",
    "from data import *\n",
    "from Trainer import *\n",
    "\n",
    "# Class for the Cup dataset\n",
    "from data_cup import *\n",
    "from Trainer_Cup import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with Stochastic Gradient Descent\n",
    " In this notebook, we have implemented a Neural Network with ***Stochastic Gradient Descent (SGD)*** with mini-batch optimization using the Keras library.\n",
    " \n",
    " SGD updates network parameters based on gradients computed from small subsets of randomly selected training data, striking a balance between computational efficiency and stochastic learning. The mini-batch optimization approach helps in reducing the variance of the parameter updates, leading to more stable convergence. \n",
    " \n",
    "Specifically, two separate problems were tackled:\n",
    " - 1) classification task, involving the MONK datasets, divided into three parts;\n",
    " - 2) regression task, on the other hand, involved the CUP dataset, with a final blind test. Our goal was to thoroughly design, implement, and evaluate neural network architectures against the assigned tasks.\n",
    "\n",
    "\n",
    "To evaluate the performance of our neural networks, we used several metrics suitable for each task. For the MONK problems, we used **Mean Square Error (MSE)** to calculate the loss and accuracy as the metric to measure classification performance. \n",
    "For the CUP dataset, we also used Mean Square Error (MSE) for calculating the loss, while **Mean Euclidean Error (MEE)** was employed as the metric to evaluate the effectiveness of the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Monk 1 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_train = MonksDataset('monk1_train')\n",
    "m1_test= MonksDataset('monk1_test')\n",
    "\n",
    "#Splitting the data into train/dev, and test sets\n",
    "X_dev, y_dev, X_test_m1, y_test_m1 = get_monks_data(m1_train, m1_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding with the O.H.E. method using pandas get_dummies()\n",
    "X_monk_train_ohe_pd_cat = pd.get_dummies(X_dev, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6'])\n",
    "X_monk_test_ohe_pd_cat = pd.get_dummies(X_test_m1, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture using only lr and momentum\n",
    "def model_creation( learning_rate, momentum ):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(4, activation='tanh', input_shape=(17,)))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  #tf.random.set_seed(54)\n",
    "  model.compile(\n",
    "        optimizer= SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters definition for the grid search\n",
    "params = {\n",
    "    'batch_size': [ 8, 16, 32],\n",
    "    'model__learning_rate': [ 0.3, 0.5, 0.7, 0.8 ],\n",
    "    'model__momentum': [0.3, 0.4, 0.5, 0.7, 0.8]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Wrap model creation in KerasClassifier\n",
    "model = KerasClassifier(model=model_creation, epochs=100, metrics='accuracy_score', random_state=42, verbose=0)\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, cv=5, error_score='raise', n_jobs=-1, verbose=0)\n",
    "grid_result = grid.fit(X_monk_train_ohe_pd_cat, y_dev, verbose=0)\n",
    "\n",
    "# Get best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation and training\n",
    "model_monk1 =model_creation(0.8,0.8)\n",
    "model_trained_m1 = Trainer(model_monk1, X_monk_train_ohe_pd_cat, y_dev, X_monk_test_ohe_pd_cat, y_test )\n",
    "model_trained_m1.train(epochs=300, batch_size=8) # Trainer functions\n",
    "model_trained_m1.plot_history('Monk1')  # Plotting of the training history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_m1.evaluate(X_monk_train_ohe_pd_cat, y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_m1.evaluate(X_monk_test_ohe_pd_cat, y_test_m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_train = MonksDataset('monk2_train')\n",
    "m2_test= MonksDataset('monk2_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_m2, y_dev_m2, X_test_m2, y_test_m2 = get_monks_data(m2_train, m2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_monk2_dev_ohe_pd_cat = pd.get_dummies(X_dev_m2, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6'])\n",
    "X_monk2_test_ohe_pd_cat = pd.get_dummies(X_test_m2, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': [8, 16, 32, 64, len(X_dev_m2)],\n",
    "    'model__learning_rate': [0.1 ,0.3, 0.5, 0.7 ],\n",
    "    'model__momentum': [  0.5, 0.7, 0.8, 0.9],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Wrap model creation in KerasClassifier\n",
    "model = KerasClassifier(model=model_creation, epochs=50, verbose=0)\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, cv=5, error_score='raise' )\n",
    "grid_result = grid.fit(X_monk2_dev_ohe_pd_cat, y_dev_m2)\n",
    "\n",
    "# Get best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_monk2 =model_creation(0.5, 0.7)\n",
    "model_trained_m2 = Trainer(model_monk2, X_monk2_dev_ohe_pd_cat, y_dev_m2, X_monk2_test_ohe_pd_cat, y_test_m2 )\n",
    "model_trained_m2.train(epochs=300, batch_size=8)\n",
    "model_trained_m2.plot_history('Monk2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_m2.evaluate(X_monk2_dev_ohe_pd_cat, y_dev_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_m2.evaluate(X_monk2_test_ohe_pd_cat, y_test_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_train = MonksDataset('monk3_train')\n",
    "m3_test= MonksDataset('monk3_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_m3, y_dev_m3, X_test_m3, y_test_m3 = get_monks_data(m3_train, m3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_monk3_dev_ohe_pd_cat = pd.get_dummies(X_dev_m3, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6'])\n",
    "X_monk3_test_ohe_pd_cat = pd.get_dummies(X_test_m3, columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': [8, 16],\n",
    "    'model__learning_rate': [ 0.01, 0.1 ,0.3, 0.5],\n",
    "    'model__momentum': [  0.7, 0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Wrap model creation in KerasClassifier\n",
    "model = KerasClassifier(model=model_creation, epochs=300, verbose=0)\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, cv=5, error_score='raise', n_jobs=-1)\n",
    "grid_result = grid.fit(X_monk3_dev_ohe_pd_cat, y_dev_m3)\n",
    "\n",
    "# Get best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_monk3 =model_creation(0.01, 0.7)\n",
    "model_trained_m3 = Trainer(model_monk3, X_monk3_dev_ohe_pd_cat, y_dev_m3, X_monk3_test_ohe_pd_cat, y_test_m3 )\n",
    "model_trained_m3.train(epochs=300, batch_size=8)\n",
    "model_trained_m3.plot_history('Monk3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_m3.evaluate(X_monk3_dev_ohe_pd_cat, y_dev_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_m3.evaluate(X_monk3_test_ohe_pd_cat, y_test_m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 3 with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation with the addition of searching for the best value of regularization\n",
    "def model_creation_regularizer( learning_rate, momentum, regularizers ):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(4, activation='tanh', input_shape=(17,),kernel_regularizer=l2(regularizers)))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(\n",
    "        optimizer= SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of hyerparameters to search\n",
    "params = {\n",
    "    'batch_size': [ 32, 64, 128],\n",
    "    'model__learning_rate': [ 0.3, 0.4, 0.5],\n",
    "    'model__momentum': [ 0.7,0.8, 0.9],\n",
    "    'model__regularizers': [ 0.01, 0.001, 0.0001]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Definition of the model\n",
    "model = KerasClassifier(model=model_creation, epochs=600, verbose=0)\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, cv=5, error_score='raise', n_jobs=-1, verbose=0)\n",
    "grid_result = grid.fit(X_monk3_dev_ohe_pd_cat, y_dev_m3)\n",
    "\n",
    "# Get best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_monk3_reg =model_creation_regularizer(0.1, 0.3, 0.01)\n",
    "model_trained_m3_reg = Trainer(model_monk3_reg, X_monk3_dev_ohe_pd_cat, y_dev_m3, X_monk3_test_ohe_pd_cat, y_test_m3 )\n",
    "model_trained_m3_reg.train(epochs=600, batch_size=32)\n",
    "model_trained_m3_reg.plot_history('Monks 3 with reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_m3_reg.evaluate(X_monk3_dev_ohe_pd_cat, y_dev_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_m3_reg.evaluate(X_monk3_test_ohe_pd_cat, y_test_m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the dataset for Cup training and Cup test\n",
    "cup = CupDataset('Cup_tr')\n",
    "blind = CupDataset('Cup_ts')\n",
    "\n",
    "blind = blind.data\n",
    "df =cup.data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Val - Test Split\n",
    " The dataset are splitted into 3 part: Train, Val, Test. The Dev set(90%), include Train(90%) and Val(10%) for model selection, and Test set(10%) is used for final evaluation for model assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation and test sets\n",
    "cup.split_data(test_size=0.1, random_state=0)\n",
    "\n",
    "# X_dev and y_dev represent the features and labels of the development set (train/validation combined), X_final_test and y_final_test represent the features and labels of the final test set\n",
    "X_dev,  X_final_test, y_dev, y_final_test = cup.get_splits()\n",
    "\n",
    "# Further split the development set (X_dev, y_dev) into training and internal test sets\n",
    "X_train, X_internal_test, y_train, y_internal_test = train_test_split(X_dev, y_dev, test_size=0.111, random_state=0)\n",
    "\n",
    "# Extract the features from the 'blind' dataset \n",
    "X_blind = blind[['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'a10']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesing\n",
    "To preprocess the data, a polynomial transformation (***PolynomialFeatures***) was applied using the polynomial degree fixed. Next, the ***arctanh*** (hyperbolic arcotangent) function was applied to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = np.arctanh(poly.fit_transform(X_train)[:,1:])\n",
    "X_internal_test_poly = np.arctanh(poly.transform(X_internal_test)[:,1:])\n",
    "X_final_test_poly = np.arctanh(poly.transform(X_final_test)[:,1:])\n",
    "X_dev_poly = np.arctanh(poly.transform(X_dev)[:,1:])\n",
    "X_blind_poly = np.arctanh(poly.transform(X_blind)[:,1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Hyperparameters tuning\n",
    "An initial broad grid search is typically performed to explore a wide array of hyperparameter values, aiming to pinpoint the most promising regions within the parameter space. Once these regions are identified, a subsequent, more detailed grid search is conducted to precisely optimize the hyperparameter settings.\n",
    "\n",
    " 1)  A ***coarse grid search***,  was initially conducted, for the model selection using primarily to determine the optimal hyperparameters for the model architecture.\n",
    " 2) A more ***fine grid search*** was performed to accurately identify the optimal learning hyperparameters.\n",
    "\n",
    "The optimal configurations for final re-training and evaluation on the internal test set are those that achieve the lowest mean MEE during cross-validation on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of function to estimate the metric MEE(Mean Euclidean Error)\n",
    "def mean_euclidean_error(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the function for the model creation to use for the grid search on the archicture parameters\n",
    "initializer = GlorotUniform(seed=12)\n",
    "in_dim = X_train_poly.shape[1]\n",
    "out_dim = y_train.shape[1]\n",
    "\n",
    "def create_mlp_model(activation, num_hidden_layers, h_units, learning_rate, momentum, regularizers):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(in_dim,)))\n",
    "\n",
    "    # # Add the hidden layers.\n",
    "    for i in range(1, num_hidden_layers+1):\n",
    "        model.add(Dense(h_units, activation=activation,kernel_initializer=initializer, kernel_regularizer=l2(regularizers)) )\n",
    "\n",
    "    model.add(Dense(out_dim, activation='linear'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer= SGD(learning_rate=learning_rate, momentum=momentum, clipnorm=10.0),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[mean_euclidean_error]\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the early stopping criteria\n",
    "early_stopping = EarlyStopping(\n",
    "                              patience=50,\n",
    "                              monitor=\"mean_euclidean_error\",\n",
    "                              mode='min',\n",
    "                              restore_best_weights=True,\n",
    "                              min_delta=0.01,\n",
    "                              verbose=1\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the hyperparameters to search for the corase grid search\n",
    "params = {\n",
    "    'batch_size': [64, 128],\n",
    "    'model__num_hidden_layers': [2,3],\n",
    "    'model__h_units': [ 60, 120, 150],\n",
    "    'model__learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'model__momentum': [0.0, 0.3, 0.6, 0.9],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__regularizers': [0.001, 0.01, 0.0001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "n_jobs_search = -1\n",
    "\n",
    "\n",
    "# Definition of the model to use for the grid search, the grid search will be done on the architecture parameters using the cross validation on 5 and MEE as scorer\n",
    "\n",
    "mlp = KerasRegressor(\n",
    "                    model=create_mlp_model,\n",
    "                    epochs=800,\n",
    "                    callbacks=[early_stopping],\n",
    "                    )\n",
    "\n",
    "mlp_cv_m1 = GridSearchCV(estimator=mlp, param_grid=params, scoring=make_scorer(lambda x, y : mean_euclidean_error(x, y).numpy()), cv=5, verbose=0,\n",
    "                        n_jobs=n_jobs_search, error_score='raise')\n",
    "\n",
    "grid_result = mlp_cv_m1.fit(X_train_poly, y_train, verbose =0 )\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "results = sorted(zip(means, stds, params), key=lambda x: x[0], reverse=False)\n",
    "for mean, stdev, param in results:\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the results of the coarse grid search are printed and sorted\n",
    "results = sorted(zip(means, stds, params), key=lambda x: x[0], reverse=False)\n",
    "for mean, stdev, param in results:\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Grid search on hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = GlorotUniform(seed=12)\n",
    "in_dim = X_train_poly.shape[1]\n",
    "out_dim = y_train.shape[1]\n",
    "# Definition of the model for the fine grid search\n",
    "def create_mlp_model_arch( learning_rate, momentum, regularizers):\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(in_dim,)))\n",
    "    for _ in range(3):\n",
    "        model.add(Dense(150, activation='tanh', kernel_initializer=initializer, kernel_regularizer=l2(regularizers)))\n",
    "    \n",
    "    model.add(Dense(out_dim, activation='linear'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=learning_rate, momentum=momentum, clipnorm=10.0),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[mean_euclidean_error]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of hyperparameters for the fine grid search\n",
    "param_dist = {\n",
    "    'batch_size': (32, 64, 128),\n",
    "    'model__learning_rate':(0.01, 0.008, 0.005, 0.003, 0.001),\n",
    "    'model__momentum': (0.5, 0.6, 0.7),\n",
    "    'model__regularizers': (0.0001, 0.0005, 0.00005, 0.0001)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(model=create_mlp_model_arch, verbose=0, epochs=800, callbacks=[early_stopping])\n",
    "\n",
    "grid_fine_search = GridSearchCV(estimator=model, param_grid=param_dist, scoring=make_scorer(lambda x, y : mean_euclidean_error(x, y).numpy()), n_jobs=-1, cv=5, verbose=3)\n",
    "grid_fine_search.fit( X_train_poly, y_train)\n",
    "\n",
    "# Stampa i risultati della random search\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_fine_search.best_params_)\n",
    "print(\"Best score: \", grid_fine_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the cross-validation results for the fine grid search and sort them by the best mean test score\n",
    "results = grid_fine_search.cv_results_\n",
    "\n",
    "sorted_indices = np.argsort(results['mean_test_score'])\n",
    "\n",
    "num_best_results = 5\n",
    "\n",
    "print(f\"Best {num_best_results} Results:\")\n",
    "for i in range(num_best_results):\n",
    "    index = sorted_indices[i]\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    print(f\"  Parameters: {results['params'][index]}\")\n",
    "    print(f\"  Mean Test Score: {results['mean_test_score'][index]:.4f}\")\n",
    "    print(f\"  Std Test Score: {results['std_test_score'][index]:.4f}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assessment\n",
    "After identifying the optimal hyperparameter configuration using the two different grid searches, we proceed with training our model on the training/validation set. This approach allows us to fully utilize all available training/validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the best model found by the fine grid search and plotting the learning curves\n",
    "model = create_mlp_model('tanh', 3, 150, 0.003, 0.7, 0.0001)\n",
    "model_trained = Trainer_cup(model, X_train_poly, y_train, X_internal_test_poly, y_internal_test, use_early_stopping=True, early_stopping_patience=50) \n",
    "model_trained.train(epochs=1200, batch_size=32, verbose=0)\n",
    "model_trained.plot_history('Model SGD with KerasRegressor')\n",
    "# Save the model withe the best Hyperparameters found\n",
    "#model_trained.save_model('model_trained_1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Average to estimate the error of the model\n",
    "\n",
    "To achieve a more precise estimate on the validation test, internal test, than on the blind test, we conduct 5 trials with identical settings and then calculate the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model trained on train of validation and estimate error on validation test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array to store predictions of development and test data for each trial\n",
    "train_preds_arr = np.zeros((5, len(y_train), 3))\n",
    "internal_test_preds_arr = np.zeros((5, len(y_internal_test), 3))\n",
    "\n",
    "# Perform 5 training trials\n",
    "for i in range(5):\n",
    "    \n",
    "    model_train = create_mlp_model('tanh', 3, 150, 0.003, 0.7, 0.0001)\n",
    "    \n",
    "    model_trained = Trainer_cup(model_train, X_train_poly, y_train,  use_early_stopping=True, early_stopping_patience=50)\n",
    "    \n",
    "    model_trained.train(epochs=1200, batch_size=32, verbose=0)\n",
    "    \n",
    "    train_preds_arr[i] = model_trained.predict(X_train_poly)\n",
    "    \n",
    "    internal_test_preds_arr[i] = model_trained.predict(X_internal_test_poly)\n",
    "    \n",
    "    \n",
    "    test_loss, test_mee = model_trained.evaluate(X_internal_test_poly, y_internal_test)\n",
    "    print(f\"Trial {i+1} - Test Loss: {test_loss:.4f}, Test MEE: {test_mee:.4f}\")\n",
    "\n",
    "\n",
    "train_preds_mean = np.mean(train_preds_arr, axis=0)\n",
    "internal_test_preds_mean = np.mean(internal_test_preds_arr, axis=0)\n",
    "\n",
    "\n",
    "print(\"Mean Dev Predictions:\")\n",
    "print(train_preds_mean)\n",
    "print(\"Mean Final Test Predictions:\")\n",
    "print(internal_test_preds_mean)\n",
    "\n",
    "Train_test_preds_df = pd.DataFrame(train_preds_mean, columns=['x', 'y', 'z'])\n",
    "Train_test_preds_df.to_csv('Train_avg_predictions.csv', index=False)\n",
    "\n",
    "internal_test_preds_df = pd.DataFrame(internal_test_preds_mean, columns=['x', 'y', 'z'])\n",
    "internal_test_preds_df.to_csv('Internal_test_avg_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- TRAIN (MEAN 5 TRIALS) --')\n",
    "train_preds = np.mean(train_preds_arr, axis=0)\n",
    "loss_train_mean = np.mean(np.square(y_train - train_preds))\n",
    "mee_train_mean = np.mean(np.sqrt(np.sum(np.square(y_train - train_preds), axis=1)))\n",
    "print(f'Mean Loss (MSE): {loss_train_mean:.4f} - Mean MEE: {mee_train_mean:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- INTERNAL TEST (MEAN 5 TRIALS) --')\n",
    "loss_internal_test_mean = np.mean(np.square(y_internal_test - internal_test_preds_mean))\n",
    "\n",
    "mee_internal_test_mean = np.mean(np.sqrt(np.sum(np.square(y_internal_test - internal_test_preds_mean), axis=1)))\n",
    "print(f'Mean Loss (MSE): {loss_internal_test_mean:.4f} - Mean MEE: {mee_internal_test_mean:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model trained on all validation set and estimate error on final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array to store predictions of development and test data for each trial\n",
    "dev_preds_arr = np.zeros((5, len(y_dev), 3))\n",
    "final_test_preds_arr = np.zeros((5, len(y_final_test), 3))\n",
    "\n",
    "# Perform 5 training trials\n",
    "for i in range(5):\n",
    "    \n",
    "    model_dev = create_mlp_model('tanh', 3, 150, 0.003, 0.7, 0.0001)\n",
    "    \n",
    "    \n",
    "    model_trained_dev = Trainer_cup(model_dev, X_dev_poly, y_dev,  use_early_stopping=True, early_stopping_patience=50)\n",
    "    \n",
    "    \n",
    "    model_trained_dev.train(epochs=1200, batch_size=32, verbose=0)\n",
    "    \n",
    "    \n",
    "    dev_preds_arr[i] = model_trained_dev.predict(X_dev_poly)\n",
    "    \n",
    "    \n",
    "    final_test_preds_arr[i] = model_trained_dev.predict(X_final_test_poly)\n",
    "    \n",
    "    \n",
    "    test_loss, test_mee = model_trained_dev.evaluate(X_final_test_poly, y_final_test)\n",
    "    print(f\"Trial {i+1} - Test Loss: {test_loss:.4f}, Test MEE: {test_mee:.4f}\")\n",
    "\n",
    "\n",
    "dev_preds_mean = np.mean(dev_preds_arr, axis=0)\n",
    "final_test_preds_mean = np.mean(final_test_preds_arr, axis=0)\n",
    "\n",
    "\n",
    "print(\"Mean Dev Predictions:\")\n",
    "print(dev_preds_mean)\n",
    "print(\"Mean Final Test Predictions:\")\n",
    "print(final_test_preds_mean)\n",
    "\n",
    "dev_test_preds_df = pd.DataFrame(dev_preds_mean, columns=['x', 'y', 'z'])\n",
    "dev_test_preds_df.to_csv('Dev_avg_predictions.csv', index=False)\n",
    "\n",
    "internal_test_preds_df = pd.DataFrame(final_test_preds_mean, columns=['x', 'y', 'z'])\n",
    "internal_test_preds_df.to_csv('Internal_test_avg_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- DEV (MEAN 5 TRIALS) --')\n",
    "dev_preds = np.mean(dev_preds_arr, axis=0)\n",
    "loss_dev_mean = np.mean(np.square(y_dev - dev_preds))\n",
    "mee_dev_mean = np.mean(np.sqrt(np.sum(np.square(y_dev - dev_preds), axis=1)))\n",
    "print(f'Mean Loss (MSE): {loss_dev_mean:.4f} - Mean MEE: {mee_dev_mean:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- FINAL INTERNAL TEST (MEAN 5 TRIALS) --')\n",
    "loss_final_test_mean = np.mean(np.square(y_final_test - final_test_preds_mean))\n",
    "\n",
    "mee_final_test_mean = np.mean(np.sqrt(np.sum(np.square(y_final_test - final_test_preds_mean), axis=1)))\n",
    "print(f'Mean Loss (MSE): {loss_final_test_mean:.4f} - Mean MEE: {mee_final_test_mean:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model trained on all the dataset and made the predictions on the blind test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tot=df[['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7','a8', 'a9', 'a10']]\n",
    "y_tot=df[['t1', 't2', 't3']]\n",
    "X_tot_poly = np.arctanh(poly.transform(X_tot)[:,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_preds_arr = np.zeros((5, 1000, 3))\n",
    "blind_preds_arr = np.zeros((5, 900, 3))\n",
    "# Perform 5 training trials\n",
    "for i in range(5):\n",
    "    # Train (early stopping on validation/internal test MEE)\n",
    "    model =  modello_2L = create_mlp_model('tanh', 3, 150, 0.003, 0.7, 0.0001)\n",
    "    model_init = Trainer_cup(model, X_tot_poly, y_tot, target='mean_euclidean_error', use_early_stopping=True, early_stopping_patience=50)\n",
    "    model_init.train(epochs=1200, batch_size=32, verbose=0)\n",
    "\n",
    "    # Total dataset predictions\n",
    "    tot_preds_arr[i] = model.predict(X_tot_poly)\n",
    "    # Blind test predictions\n",
    "    blind_preds_arr[i] = model.predict(X_blind_poly)\n",
    "    \n",
    "\n",
    "blind_average_preds = np.mean(blind_preds_arr, axis=0)\n",
    "blind_test_preds_df = pd.DataFrame(blind_average_preds, columns=['x', 'y', 'z'])\n",
    "blind_test_preds_df.to_csv('Blind_test_avg_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
